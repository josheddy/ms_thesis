\chapter{Future Work}

\section{System Improvements}

Experimentation with the \texttt{kalman\_sense} ROS package has revealed a number of areas for improvement. Speaking generally, the system needs enhancements to improve accuracy and to become more robust with respect to sensor malfunction. Further development of this work likely would center on three main areas:
\begin{enumerate}
    \item Integration of new sensors,
    \item Improvements for current sensors, and
    \item Refinements to the underlying algorithm.
\end{enumerate}
Modifications to any or all of these areas would produce a marked improvement to the system in terms of both accuracy and robustness. For maximal effectiveness in adverse conditions, the system's sensors would need to be both individually robust and collectively redundant, so that complementary sensors could compensate for the loss of another sensor's information.

\subsection{Integration of New Sensors}
The accuracy and robustness of the system could be augmented by the integration of additional sensors, including but not limited to, the following:
\begin{enumerate}
    \item One or more GPS receivers (particularly Real Time Kinematic\footnote{\url{https://en.wikipedia.org/wiki/Real_Time_Kinematic}} GPS),
    \item A pressure altimeter,
    \item A 3-axis magnetometer,
    \item One or more laser rangefinders,
    \item One or more forward-facing cameras (preferably depth cameras, such as RGB-D),
    \item One-dimensional, multi-dimensional, or scanning LIDAR\footnote{``Light Detection and Ranging,'' \url{https://en.wikipedia.org/wiki/Lidar}}.
\end{enumerate}

\begin{comment}
\begin{center}
\begin{tabular}{ m{0.25\textwidth} | m{0.65\textwidth} }
 GPS & Position (and orientation, if using multiple receivers for differential GPS) \\
 Pressure Altimeter & Altitude \\
 3-Axis Magnetometer & Heading angle \\
 Ventral Laser Rangefinder & Altitude \\
 Longitudinal/Lateral Laser Rangefinders & Distance to obstacles \\
 Forward-Facing Camera & Roll angle, distance to obstacles \\
 1D LIDAR & Similar to Laser Rangefinder \\
 Multi-dimensional/Scanning LIDAR & Full 3D map of surroundings, distance to obstacles
\end{tabular}
\end{center}
\end{comment}

We previously mentioned work from the GRASP Lab (\cite{Shen2011}) that demonstrated the efficacy of similar UKF frameworks in indoor-outdoor transitions and in navigating confined spaces using an expanded suite of sensors. In the case where more than one sensor is used to observe a given vehicle state variable, those multiple sensors check one another and can even perform real-time, in-air calibration. The system developed in this thesis has the advantage of being hardware-minimal in that it depends on only two sensors (the IMU and the ventral camera), but this convenience comes at the cost of robustness because either sensor presents a single point of possible failure. Moreover, the sensors have no overlap. Neither can observe any of the states observed by the other. This eliminates the possibility of checking one sensor against the other. Introducing a ventral laser rangefinder or pressure altimeter would allow for much more precise measurement of the vehicle's altitude and would allow for in-flight re-initialization of any metric SLAM/VO algorithm.

With the exception of certain sensors such as scanning LIDAR, which can cost tens of thousands of dollars, and RTK-GPS, which often falls in the same price range, these proposed new sensors would be inexpensive additions to the vehicle. Given the sophistication of current miniaturized sensors, it is not unreasonable to expect sub-meter or centimeter-level position accuracy from vehicles weighing fewer than fifteen pounds (including a substantial sensor payload) and costing less than 20,000~USD. It has been the case for a number of years now that meter-level accuracy could be taken for granted with ``toy'' quadcopters using only MEMS\footnote{``Microelectromechanical systems,''\\ \url{https://en.wikipedia.org/wiki/Microelectromechanical_systems}} IMUs and sub-\$100 GPS receivers. The trend of miniaturization has made more and more sensors of better and better quality increasingly available in the weight- and cost-constrained world of aerial robotics. A variety of inexpensive off-the-shelf autopilots, such as the Pixhawk~2.1\footnote{\url{http://www.proficnc.com/}}, already boast triple-redundant IMUs and support for multiple GPS modules. Many of the hardware integration challenges that once plagued hobbyists and researchers have been resolved by engineering developments stemming from popular demand for user-friendly drones.

The organization of the \texttt{kalman\_sense} package's code base is such that the integration of new sensors would require only the addition of new callback functions and ROS subscribers, as opposed to a full redesign or major edits to the existing code. Further, a new SLAM or VO algorithm could also be used in lieu of PTAM, allowing an ``apples-to-apples'' comparison of different visual localization algorithms. For example, Donavanik et~al.\ \cite{Donavanik2016} have identified a new algorithm known as ORB-SLAM \cite{Mur-Artal2015} as a promising candidate for robust SLAM, already implemented in ROS. Because of ROS's sophisticated ecosystem of interoperable packages and message formats, any pose sensor employing the same message type as PTAM could plug in to \texttt{kalman\_sense} in a matter of seconds.

Much of the design thinking that went into the \texttt{kalman\_sense} package was focused on this particular possibility. The aircraft of the future will be vehicles laden with a vast suite of heterogeneous sensors, the outputs of which would, at any given time, be used not only for inner-loop and outer-loop flight control, but also for self-calibration, real-time diagnostics, and operator telemetry. Future UAVs will be akin to floating laboratories, carrying numerous means of measurement for all manner of self-sensing as well as scientific applications.

\subsection{Improvements for Current Sensors}

For improved pose measurements, a more robust implementation of PTAM could be written and the same experiments could be repeated for comparison. PTAM's rotation errors and inability to eliminate lens distortion pose severe limitations to its effectiveness in small UAS navigation. Moreover, if a rewritten formulation were able to interface with a downward-facing rangefinder, the initialization process could be eliminated altogether. The drawback is that making these changes would require a major software rewrite, a project that could be feasibly undertaken only by an experienced team of computer vision researchers and software developers. The mere cost in terms of both time and labor would likely make this an unappealing proposition, especially if other ready-made SLAM/VO algorithms were available to serve as PTAM replacements.

Processing power will also be central to the system's effectiveness during deployment on a real aircraft. This research is in many ways the product of recent advances in computing power. The advent of miniature desktop computers, such as the Intel~NUC\footnote{``Next Unit of Computing,'' \url{https://en.wikipedia.org/wiki/Next_Unit_of_Computing}}, as well as power-efficient Graphics Processing Units\footnote{\url{https://en.wikipedia.org/wiki/Graphics_processing_unit}} (GPUs) has made a new frontier of estimation and localization algorithms accessible for the first time in the UAV world. A simple and generally inexpensive approach to augmenting the \texttt{kalman\_sense} system would thus be the inclusion of top-of-the-line lightweight computing hardware. With modern onboard computers, even the need for cross-platform code compilation (so-called ``transpiling'') is eliminated because these computers employ the same x86 architecture used by most software developers.

\subsection{Refinements to the Underlying Algorithm}

The UKF formulation itself presents another set of opportunities for improvement. In particular, better noise modeling would allow for increased accuracy. As described previously, the noise covariance matrices used during the experiments were only rough, static models meant to overestimate the noise in the system. In practice, it would be better to replace these with time-varying matrices with terms more closely matching the intrinsic parameters of the system. Due to the high degree of coupling between state variables and the fundamental nonlinearity of the process model, this type of work was considered to be outside the scope of my thesis. In the future, however, a better $\mathbf{Q}$ matrix could be determined, perhaps by application of autocovariance and system identification methods. Moreover, any new sensor integrated into the system would require its own $\mathbf{R}$ matrix detailing its particular idiosyncrasies. With this done, attention could then be turned to tuning the $\mathbf{Q} / \mathbf{R}$ ratio of each sensor, thus making the system even more faithful to known sensor parameters. With more sensors, self-calibration techniques such as those employed in \cite{Weiss2012} could also be implemented in order to avoid reinitializing one or more sensors in mid-air.

\section{Future Experiments}

Using a SLAM/VO algorithm that addresses pure rotation and lens distortion would be the easiest and most cost effective way to improve the system. Testing different visual localization algorithms would thus be an appealing avenue of inquiry. It would behoove the researcher to characterize the system's behavior under the same conditions that caused the PTAM-based implementation to fail. In particular, experiments that include pure rotations and longer translations, as well as motion over uneven but otherwise virtually unchanging terrain would mimic real-world flight missions more closely and provide a more holistic understanding of the system's effectiveness. Additionally, mounting the sensor suite on the underside of a UAV would allow for the collection of real-world flight data, which could shed light on the system's speed limits and behavior in the presence of in-flight vibrations.

After integrating new sensors, new experiments could be devised to characterize the system's effectiveness during sensor blackouts---for example, during an indoor-outdoor transition after integrating a GPS receiver. Experiments such as this could then inform the design of heuristic models for recognizing sensor degradation and perhaps even consensus-based estimation strategies. 

\section{System Applications}

Applications for this system are wide-ranging, spanning virtually every conceivable use for a small drone. The following is a non-exhaustive list of possible applications:
\begin{enumerate}
    \item Mapping and 3D reconstruction of structures and topography,
    \item Emergency response,
    \item Infrastructure inspection,
    \item Autonomous package delivery, and
    \item Military/defense solutions.
\end{enumerate}
What all of these applications have in common is the need for robust localization. In each of the above scenarios, losing GPS could cause a crash. In the event of such an anomaly, the vehicle would pose a real threat to bystanders and property. Moreover, in the case of military aircraft, the vehicle could be captured by hostile forces.

\subsubsection{Mapping and 3D Reconstruction}

Growing demand for so-called ``precision agriculture'' has brought with it the need to map large areas of cropland at a high speed, with a low cost point. In the past, this need has been met by using satellite imagery and human-captured photography. With the arrival of low-cost drones, this work has been offloaded to aerial robots. Outfitted with specialized sensors such as hyperspectral cameras\footnote{\url{https://en.wikipedia.org/wiki/Hyperspectral_imaging}}, small UAS are able to provide minute-by-minute coverage of large expanses of land. More interesting still, these robots can be outfitted with reservoirs of pesticide or fertilizer and deployed to spray individual plants autonomously. It is in roles such as this that high-accuracy robust localization becomes a necessity. The accuracy required for precision agriculture can be provided by utilizing the sensor fusion techniques discussed in this thesis.

The surfaces of buildings and other structures can be mapped in a similar fashion, given the appropriate sensors and the right framework by which to fuse them. 3D reconstruction has become popular among real estate agents, safety inspectors, and other parties with a vested interest in precise 3D rendering of large geometries. In this case, the aircraft must be able to localize itself precisely in order to take full advantage of onboard sense-and-avoid technologies. The ability of the aircraft to estimate its position is crucial to its ability to map hard-to-reach areas and maintain stable flight. Sensor fusion systems shine when GPS and other sensors are inevitably compromised by various environmental factors.

\subsubsection{Emergency Response}

UAS technologies have become a desirable tool for first responders in many parts of the world. The ability to deploy a UAV to survey forest fires or search for missing persons has been a game changer for law enforcement and emergency personnel. In the case of looking for survivors of a natural disaster, the requirements for an effective vehicle system are accurate localization and overall robustness. Vehicles sent into disaster areas will have to be able to navigate in sensor-compromised environments, sometimes at substantial speed. This use case highlights the need for redundant sensors and intelligent, failure-resistant sensor fusion.

\subsubsection{Infrastructure Inspection}

Infrastructure inspection is similar in several respects to 3D mapping. In both cases, small drones are preferable to manned aircraft because of their low cost, high maneuverability, and ease of use. In recent years, governments and private corporations have taken an interest in using small aircraft to automate the inspection of many types of infrastructure, including the following:
\begin{enumerate}
    \item Power lines,
    \item Oil rigs,
    \item Gas pipelines,
    \item Railroads, and
    \item Buildings.
\end{enumerate}
UAVs are an obvious ally to companies servicing most types of static infrastructure. Inspecting miles of wires or railroad track is a task well-suited to today's UAVs, which commonly ship with intuitive flight planning software. Operators---even those who have no experience flying remote-control aircraft---are now able to program missions and deploy drones with ease. The aircraft are able to survey miles of infrastructure regardless of conditions on the ground, such as flooding, ill-maintained roads, or wild animals. Operators with First-Person View (FPV) hardware can watch video streaming live from the aircraft as if they were onboard themselves. The need for robust localization here is obvious: the UAV cannot inspect anything to which it cannot navigate. Operating in the wilderness may require flying below the forest canopy or under natural overhangs that could block GPS reception. The same is true in urban environments, where tall buildings create so-called ``urban canyons'' which compromise GPS effectiveness. Large metal structures can also distort magnetometry readings and thus undermine the aircraft's ability to maintain its heading. In any case, UAVs used for large-scale inspections will be dependent upon intelligently combined sensing modalities.

\subsubsection{Autonomous Package Delivery}

A number of organizations, most notably Amazon\footnote{\url{https://www.amazon.com/Amazon-Prime-Air/b?node=8037720011}}, have made big bets on drones as the future of delivery services. The task of delivering goods to individual homes is no trifling matter. American homes come in all shapes and sizes and are surrounded by numerous structures and natural obstacles that could endanger a delivery aircraft and, by extension, the people and property nearby. Moreover, any drone delivery service would have to be predicated upon the ability to land with sub-meter accuracy. In scenarios such as this, visual navigation will be extremely important. Delivery drones will have to land precisely in cluttered environments amid dynamic obstacles. GPS alone will not be enough to guide the vehicle safely in every conceivable circumstance. 

\subsubsection{Military/Defense Solutions}

Perhaps the most obvious application of a UKF fusion framework is in military operations, where precision navigation and robustness to sensor degradation are paramount for protecting soldiers in the field. As of the time of this writing, UAVs have been used in combat for years. However, augmented navigation could still make revolutionary contributions in areas such as
\begin{enumerate}
    \item Robust tracking of friendly and hostile elements,
    \item Remote observation of roads and vehicle-related hazards,
    \item Autonomous deliveries of materiel in-theater, and
    \item Human-machine teaming.
\end{enumerate}
Every combat zone will constitute a hostile environment for UAV operations, especially in the presence of GPS spoofing/jamming technologies. Combat UAVs will have to be able to tolerate multiple simultaneous sensor failures. Moreover, urban flight operations bring with them all of the same difficulties mentioned above, but with the added challenge of violent enemy intercession. Robust sensor fusion could automate the task of operational overwatch and provide mission-critical intelligence in a timely manner---but only if the vehicle can localize itself reliably in sensor-hostile environments.