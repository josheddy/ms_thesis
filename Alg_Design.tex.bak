\chapter{Algorithm Design and Implementation} \label{ch:alg_design}

\section{UKF Formulation}

\subsection{Prediction Step}

We begin by defining the following quantities:
%
\begin{align} \label{eq:state_vars}
\mathbf{p} &= \left\lbrace x, y, z \right\rbrace ^{T} \\
\mathbf{q} &= \left\lbrace q_{x}, q_{y}, q_{z}, q_{w} \right\rbrace ^{T} \\
\mathbf{v} &= \left\lbrace \dot{x}, \dot{y}, \dot{z} \right\rbrace ^{T} \\
\mathbf{\Omega} &= \left\lbrace \omega_{x}, \omega_{y}, \omega_{z} \right\rbrace ^{T} \\
\mathbf{a} &= \left\lbrace \ddot{x}, \ddot{y}, \ddot{z} \right\rbrace ^{T}
\end{align}
%
The vectors $\mathbf{p}$, $\mathbf{v}$, and $\mathbf{a}$ represent the vehicle's position, velocity, and acceleration, respectively. The quaternion\footnote{From here on, all quaternion quantities are represented according to the convention $\mathbf{q} = \left\lbrace q_{x}, q_{y}, q_{z}, q_{w} \right\rbrace ^{T}$, where $q_{w}$ is the scalar component and is always placed in the last position. This convention was chosen to maintain consistency with the Eigen library's internal representation of quaternions.} $\mathbf{q}$ represents the vehicle's orientation and the vector $\mathbf{\Omega}$ represents the vehicle's angular rates. 

Let $n = 16$ be the number of state variables (presented in Equation~\ref{eq:state_vars}). We now define the state vector $\mathbf{x} \in \mathbb{R}^{n}$ as:
%
\begin{equation}
\mathbf{x} = 
\left\lbrace
    \mathbf{p}^{T},
    \mathbf{q}^{T},
    \mathbf{v}^{T},
    \mathbf{\Omega}^{T},
    \mathbf{a}^{T}
\right\rbrace ^{T}.
\end{equation}

We now define the following constants:
%
\begin{align}
\alpha &= 10^{-3} \nonumber \\
\kappa &= 0 \\
\beta &= 2 \nonumber \\
\lambda &= \alpha^{2} \left( n + \kappa \right) - n \nonumber
\end{align}
%
The constants $\alpha$ and $\kappa$ control the spread of sigma points chosen within the filter. The value of $\beta$ governs what distribution is assumed for the states $\mathbf{x}$. Setting $\beta = 2$ is optimal for a Gaussian state distribution, which we assume throughout. With these constants defined, we can now describe the selection of sigma points.

Let $\mathbf{P} \in \mathbb{R}^{n \times n}$ be the covariance matrix associated with the state $\mathbf{x}$. A set of $2n + 1$ sigma points is then derived from $\mathbf{x}$ and $\mathbf{P}$ as
%
\begin{align}
\mathbf{\chi}^{0}_{k-1 | k-1} &= \mathbf{x}_{k-1 | k-1} \nonumber\\
\mathbf{\chi}^{i}_{k-1 | k-1} &= \mathbf{x}_{k-1 | k-1} + \left( \sqrt{\left( n + \lambda \right) \mathbf{P}_{k-1 | k-1}} \right)_{i}, \quad i = 1, \dots, n \\
\mathbf{\chi}^{i}_{k-1 | k-1} &= \mathbf{x}_{k-1 | k-1} - \left( \sqrt{\left( n + \lambda \right) \mathbf{P}_{k-1 | k-1}} \right)_{i-n}, \quad i = n+1, \dots, 2n, \nonumber
\end{align}
%
where $\mathbf{\chi}^{i}$ is the $i$-th sigma point and $\left( \sqrt{\left( n + \lambda \right) \mathbf{P}} \right)_{i}$ is the $i$-th column of the matrix $\sqrt{\left( n + \lambda \right) \mathbf{P}}$. The matrix square root $\mathbf{A}$ of a matrix $\mathbf{B}$ is in turn defined here as
%
\begin{equation}
\mathbf{A} \mathbf{A}^{T} = \mathbf{B}
\end{equation}
%
and is computed via Cholesky decomposition.

To predict the next state, these sigma points are propagated through the nonlinear process function $f$ (defined later):
%
\begin{equation}
\mathbf{\chi}^{i}_{k | k-1} = f \left( \mathbf{\chi}^{i}_{k-1 | k-1} \right), \quad i = 0, \dots, 2n.
\end{equation}
%
These new sigma points are then used to predict the next state and covariance:
%
\begin{align}
\hat{\mathbf{x}}_{k | k-1} &= \sum^{2n}_{i=0} W^{i}_{s} \mathbf{\chi}^{i}_{k | k-1} \\
\mathbf{P}_{k | k-1} &= \sum^{2n}_{i=0} W^{i}_{c} \left( \mathbf{\chi}^{i}_{k | k-1} - \hat{\mathbf{x}}_{k | k-1} \right) \left( \mathbf{\chi}^{i}_{k | k-1} - \hat{\mathbf{x}}_{k | k-1} \right)^{T},
\end{align}
%
where the state weights $W_{s}$ and covariance weights $W_{c}$ are defined as
%
\begin{align}
W^{0}_{s} &= \dfrac{\lambda}{n + \lambda} \nonumber \\
W^{0}_{c} &= \dfrac{\lambda}{n + \lambda} + \left( 1 - \alpha^{2} + \beta \right) \\
W^{i}_{s} &= W^{i}_{c} = \dfrac{1}{2 \left(n + \lambda \right)} . \nonumber
\end{align}

\subsection{Correction Step}

Once again, $2n + 1$ sigma points are computed as
%
\begin{align}
\mathbf{\chi}^{0}_{k | k-1} &= \mathbf{x}_{k | k-1} \nonumber\\
\mathbf{\chi}^{i}_{k | k-1} &= \mathbf{x}_{k | k-1} + \left( \sqrt{\left( n + \lambda \right) \mathbf{P}_{k | k-1}} \right)_{i}, \quad i = 1, \dots, n \\
\mathbf{\chi}^{i}_{k | k-1} &= \mathbf{x}_{k | k-1} - \left( \sqrt{\left( n + \lambda \right) \mathbf{P}_{k | k-1}} \right)_{i-n}, \quad i = n+1, \dots, 2n. \nonumber
\end{align}
%
Next, the sigma points are projected through the observation function $h$ (defined later):
%
\begin{equation}
\mathbf{\gamma}^{i}_{k} = h \left( \mathbf{\chi}^{i}_{k | k-1} \right), \quad i = 0, \dots, 2n.
\end{equation}
%
The predicted measurement $\hat{\mathbf{z}}_{k}$ and measurement noise covariance $\mathbf{P}_{zz}$ are then computed as
%
\begin{align}
\hat{\mathbf{z}}_{k} &= \sum^{2n}_{i=0} W^{i}_{s} \mathbf{\gamma}^{i}_{k} \\
\mathbf{P}_{zz} &= \sum^{2n}_{i=0} W^{i}_{c} \left( \mathbf{\gamma}^{i}_{k} - \hat{\mathbf{z}}_{k} \right) \left( \mathbf{\gamma}^{i}_{k} - \hat{\mathbf{z}}_{k} \right)^{T} 
\end{align}
%
The state-measurement cross-covariance $\mathbf{P}_{xz}$ is then
%
\begin{equation}
\mathbf{P}_{xz} = \sum^{2n}_{i=0} W^{i}_{c} \left( \mathbf{\chi}^{i}_{k | k-1} - \hat{\mathbf{x}}_{k | k-1} \right) \left( \mathbf{\gamma}^{i}_{k} - \hat{\mathbf{z}}_{k} \right)^{T} .
\end{equation}
%
The Kalman gain $\mathbf{K}_{k}$ is then computed as
%
\begin{equation}
\mathbf{K}_{k} = \mathbf{P}_{xz} \mathbf{P}^{-1}_{zz}.
\end{equation}
%
The corrected state $\hat{\mathbf{x}}_{k | k}$ is then the sum of the predicted state and the innovation weighted by the Kalman gain:
%
\begin{equation}
\hat{\mathbf{x}}_{k | k} = \hat{\mathbf{x}}_{k | k-1} + \mathbf{K}_{k} \left( \hat{\mathbf{z}}_{k} - \mathbf{z}_{k} \right) .
\end{equation}
%
The corrected covariance $\mathbf{P}_{k | k}$ is the difference between the predicted covariance and the predicted measurement covariance, weighted by the Kalman gain:
%
\begin{equation}
\mathbf{P}_{k | k} = \mathbf{P}_{k | k-1} - \mathbf{K}_{k} \mathbf{P}_{k | k-1} \mathbf{K}_{k}^{T} .
\end{equation}


\section{Software Design Considerations}

Much of the impetus for creating the \texttt{kalman\_sense} package came from a desire to create a generic UKF framework for estimating the state of an arbitrary system using any number of relative and absolute sensors. To achieve this, the \texttt{kalman\_sense} package is organized in an object-oriented manner around an overarching abstract class called \texttt{UnscentedKf}. This abstract class contains a number of methods performing the different mathematical operations defined in Section~\ref{sec:ukf_formulation}. These methods have been written in a generic manner to enable easy extension of \texttt{UnscentedKf} by other subclasses containing concrete implementations of various systems. Currently, the package contains exactly one such subclass, known as \texttt{QuadUkf}. This subclass contains methods and data structures related directly to estimating the state of a quadcopter or other rotorcraft UAV.

This object-oriented architecture allows for a certain degree of system agnosticism. By this, we mean that the \texttt{UnscentedKf} class encapsulates the generic mathematics of the UKF without knowledge of particular system constraints. This class does little other than matrix mathematics and is designed to take as input the number of a system's states $n$ and its number of sensors $m$. With this knowledge, \texttt{UnscentedKf} is able to populate a set of mean and covariance weights and intelligently perform all of the requisite linear algebra for the UKF formulation. All other knowledge of particular states, sensors, vehicle geometry, and other metrics is hidden within subclasses such as \texttt{QuadUkf}.

\texttt{UnscentedKf} behaves in a manner similar to a Java interface in that it requires the extending class to supply functions codifying a process model and an observation model for the system under scrutiny. These two functions, along with $n$ and $m$, form the entirety of what \texttt{UnscentedKf} ``knows about the vehicle.'' All other details, including the fact that the class is being used in a ROS environment, are hidden from \texttt{UnscentedKf}. It is worth noting that \texttt{UnscentedKf}'s only dependency is on the Eigen\footnote{\url{www.eigen.tuxfamily.org}} C++ linear algebra library.

The subclass (\texttt{QuadUkf} for the remainder of this thesis) handles all of the ROS communications for the given system. Specifically, this class has callback functions for receiving sensor data and is responsible for publishing state and covariance estimates. The \texttt{kalman\_sense} \texttt{main} method handles setup and teardown of the necessary ROS publisher and subscriber nodes.

