\chapter{Introduction}

Until recently, the sensing and localization capabilities of small unmanned aerial vehicles (UAVs) have been computationally constrained due to restrictions on both the size and weight of sensors and onboard computer systems. Recent advances in miniaturized desktop computers, as well as low-power Graphics Processing Units (GPUs), have enabled a new class of computationally intensive algorithms to run onboard small aircraft without degrading flight time or other performance metrics. Specifically, small drones now present a viable platform for high-fidelity Simultaneous Localization and Mapping (SLAM), visual object recognition, and state estimation algorithms employing numerous heterogeneous sensors. In addition, the rising popularity of drones among hobbyists and researchers has fueled tremendous growth in the markets for brushless motors, electronic speed controllers, and airframes. With high-quality flight hardware and lightweight computers readily available, the arsenal of the robotics researcher has never been better stocked. The work detailed in this thesis is, in many ways, the product of these advantageous market conditions. With small aircraft capable of lifting heavier payloads and computers now lighter and smaller in footprint, demand is high for outfitting high-performance vehicles with top-notch sensing capabilities.

This work centers on an algorithm known as the Unscented Kalman Filter (UKF), a sensor fusion method for estimating the state of systems such as small aircraft. For years, this algorithm was inaccessible to roboticists and aerospace researchers alike due to its computational complexity. Computers capable of running the algorithm were simply too large, too heavy, and too power-hungry to be viable onboard components of small UAVs. The advent of smaller computers has changed this state of affairs. In this thesis, we present a formulation of the Unscented Kalman Filter aimed at estimating the pose of a quadcopter. This UKF formulation fuses outputs from only two sensors: an inertial measurement unit (IMU) and a downward-facing monocular camera. These two data streams are fused to estimate the vehicle's pose in real time. We propose this UKF framework as a hardware-minimal starting point for advanced multi-sensor fusion in UAV navigation.

\section{System Applications}

Applications for this system are wide-ranging, spanning virtually every conceivable use for a small drone. The following is a non-exhaustive list of possible applications:
\begin{enumerate}
    \item Mapping and 3D reconstruction of structures and topography,
    \item Emergency response,
    \item Infrastructure inspection,
    \item Autonomous package delivery, and
    \item Military/defense solutions.
\end{enumerate}
What all of these applications have in common is the need for robust localization. In each of the above scenarios, losing GPS could cause a crash. In such an event, the vehicle itself would pose a real threat to bystanders and property. Moreover, in the case of military aircraft, the vehicle could be captured by hostile forces.

\subsubsection{Mapping and 3D Reconstruction}

Growing demand for so-called ``precision agriculture'' has brought with it the need to map large areas of cropland at a high speed, with a low cost point. In the past, this need has been met by using satellite imagery and human-captured photography. With the arrival of low-cost drones, this work has been offloaded to aerial robots. Outfitted with specialized sensors such as hyperspectral cameras\footnote{\url{https://en.wikipedia.org/wiki/Hyperspectral_imaging}}, small UAS are now able to provide minute-by-minute coverage of large expanses of land. Moreover, these robots can be outfitted with reservoirs of pesticide or fertilizer and deployed to spray individual plants autonomously. It is in such roles that high-accuracy robust localization becomes a necessity. The accuracy required for precision agriculture can be obtained by utilizing the sensor fusion techniques discussed in this thesis.

The surfaces of buildings and other structures can be mapped in a similar fashion, given the appropriate sensors and the right framework by which to fuse them. 3D reconstruction has become popular among real estate agents, safety inspectors, and other parties with a vested interest in precise 3D renderings of large geometries. In this case, the aircraft must be able to localize itself precisely in order to take full advantage of onboard sense-and-avoid technologies. The ability of the aircraft to estimate its position is crucial to its ability to map hard-to-reach areas and maintain stable flight. Sensor fusion systems shine when GPS and other sensors are inevitably compromised by various environmental factors.

\subsubsection{Emergency Response}

UAS technologies have become a desirable tool for first responders in many parts of the world. The ability to deploy a UAV to survey forest fires or search for missing persons has been a game changer for law enforcement and emergency response teams. In the case of looking for survivors of a natural disaster, the requirements for an effective vehicle system are accurate localization and overall robustness. Vehicles sent into disaster areas will have to be able to navigate in sensor-compromising environments, sometimes at substantial speed. This use case highlights the need for redundant sensors and intelligent, failure-resistant sensor fusion.

\subsubsection{Infrastructure Inspection}

Infrastructure inspection is similar in many respects to 3D mapping. In both cases, small drones are preferable to manned aircraft because of their low cost, high maneuverability, and ease of use. Governments and private corporations alike have taken an interest in using small aircraft to automate the inspection of many types of infrastructure, including the following:
\begin{enumerate}
    \item Power lines,
    \item Oil rigs,
    \item Gas pipelines,
    \item Railroads, and
    \item Buildings.
\end{enumerate}
UAVs are an obvious ally to companies servicing most types of static infrastructure. Inspecting miles of wires or railroad track is a task well-suited to today's UAVs, which commonly ship with intuitive flight planning software. Operators---even those who have no experience flying remote-control aircraft---are now able to program missions and deploy drones with ease. The aircraft are able to survey miles of infrastructure regardless of conditions on the ground, such as flooding, ill-maintained roads, or wild animals. Operators with First-Person View (FPV) hardware can watch video streaming live from the aircraft as if they were onboard themselves. The need for robust localization here is obvious: the UAV cannot inspect anything to which it cannot navigate. Operating in the wilderness may require flying below the forest canopy or under natural overhangs that could block GPS reception. The same is true in urban environments, where tall buildings create so-called ``urban canyons'' which compromise GPS effectiveness. Large metal structures can also distort magnetometry readings and thus undermine the aircraft's ability to maintain its heading. In any case, UAVs used for large-scale inspections will be dependent upon intelligently combined sensing modalities.

\subsubsection{Autonomous Package Delivery}

A number of organizations, most notably Amazon\footnote{\url{https://www.amazon.com/Amazon-Prime-Air/b?node=8037720011}}, have made big bets on drones as the future of delivery services. The task of delivering goods to individual homes is no trifling matter. American homes come in all shapes and sizes and are surrounded by numerous structures and natural obstacles that could endanger a delivery aircraft and, by extension, the people and property nearby. Moreover, any drone delivery service would have to be predicated upon the ability of the UAV to land with sub-meter accuracy. Delivery drones will have to land precisely in cluttered environments amid dynamic obstacles. Visual sensing will be extremely important for identifying safe landing zones, avoiding power lines and fences, and surmounting other environmental challenges. GPS alone will not be enough to guide the vehicle safely in every conceivable circumstance. 

\subsubsection{Military/Defense Solutions}

Perhaps the most obvious application of a UKF fusion framework is in military operations, where precision navigation and robustness to sensor degradation are paramount for protecting soldiers in the field. As of the time of this writing, UAVs have been used in combat for years. However, augmented navigation could still make revolutionary contributions in areas such as
\begin{enumerate}
    \item Robust tracking of friendly and hostile elements,
    \item Remote observation of roads and vehicle-related hazards,
    \item Autonomous deliveries of materiel in-theater, and
    \item Human-machine teaming.
\end{enumerate}
Every combat zone will constitute a hostile environment for UAV operations, especially in the presence of GPS spoofing/jamming technologies. Combat UAVs will have to be able to tolerate multiple simultaneous sensor failures. Moreover, in-theater urban flight operations bring with them all of the same difficulties mentioned above, but with the added challenge of violent enemy interdiction. Robust sensor fusion could automate the task of operational overwatch and provide mission-critical intelligence in a timely manner---but only if the vehicle can localize itself reliably in sensor-hostile environments.

\section{Personal Motivation}

\textit{This section is largely comprised of background information regarding certain experiences leading up to, and motivating, the research developed in my thesis. This section does not contain any technical material, and is included only to provide context to the larger work.}

\vspace{1em}

\begin{wrapfigure}{r}{0.4\textwidth}
  \centering
    \includegraphics[width=0.4\textwidth]{CARD_gearREALLYBIG}
  \caption[CARD Team Logo]{The CARD team logo.}
  \label{fig:card_logo}
\end{wrapfigure}

I first took an interest in unmanned aircraft in the fall of 2012, my sophomore year of college. Several of my friends and I founded the Cooperative Autonomous Robotics Design (CARD) team at Virginia Tech in order to pursue our shared interest in autonomous vehicles. Our core team consisted of a dozen students devoted to designing and competing with drones and other robotic vehicles. Our team, guided by my future graduate adviser, Dr.~Kevin Kochersberger, entered two design competitions and brought home two awards for the university. We were also one of four university robotics teams selected by the Smithsonian Institution to participate in the opening ceremony for Robotics Week~2015. These early experiences with the team brought me into contact with new skills such as microcontroller programming, Proportional-Integral-Derivative (PID) controller design, basic mechatronics, and computer-aided design (CAD) modeling.

After two years of involvement with the CARD team, I applied for an internship at the National Institute of Aerospace\footnote{\url{http://www.nianet.org}} (NIA) in Hampton, Virginia. In the summer of 2014, I was part of a team of NIA researchers working on the Flying Donkey Challenge\footnote{\url{http://www.flyingdonkey.org}}, an international engineering competition centered around the idea of ``flying donkeys,'' full-sized autonomous airplanes capable of quickly carrying cargo between small airports in rural Africa. This competition, unfortunately now defunct, was divided into a number of sub-challenges focusing on different technical objectives such as precision landing and collision avoidance. Our team's goal was to design an inexpensive navigation system that could reliably guide unmanned aircraft during a Global Positioning System (GPS) blackout. This project introduced me to many of the technologies and techniques that would later become my major research interests, particularly the Robot Operating System\footnote{\url{http://wiki.ros.org}} (ROS), Kalman Filtering, and sensor fusion.

Through my internship at the NIA, I met Dr.~Danette Allen, head of the NASA Langley Autonomy Incubator. During my 2014--15 academic year, Dr.~Allen sponsored the CARD team to design and build two autonomous multirotor delivery drones. These aircraft were capable of delivering 5\nobreakdash-lb packages to distances of up to 2.5~miles (or 5~miles, round trip). In addition, these vehicles were able to land precisely on 1~m$^2$ April tags, a type of visual marker used in augmented reality applications. Following the completion of this project, I worked as a summer intern at the Autonomy Incubator, thereby further advancing my interest in Kalman filtering, sensor fusion, and visual localization.

During the summer of 2015, I began the research that evolved into my thesis project, studying Visual-Inertial Navigation (VIN) and the Unscented Kalman Filter (UKF). As I read more and more on these subjects, I became interested in the design of the algorithm underlying the UKF. Unlike many other formulations of the Kalman Filter, the UKF has a notably limited dependence on information about the system under scrutiny (this \textit{system agnosticism} is discussed in more detail in Chapter~\ref{ch:Alg_Design}). The more I learned about the UKF, the more I became excited about the idea of taking advantage of this limited dependence trait to build a minimalistic software interface by which a wide variety of disparate systems could be tracked and studied in a ROS framework. I envisioned a ``one-stop shopping'' experience for massively reusable and customizable filtering profiles that could fulfill the needs of researchers and roboticists who may have little knowledge of state estimation techniques. This vision eventually drove my development of the \texttt{kalman\_sense} ROS package, cementing my interest in unmanned aerial vehicle (UAV) state estimation processes and controls.


\section{Organization of this Document}

\subsection*{Prior Work}

In Prior Work, we explore recent contributions to loosely coupled filter-based navigation and state estimation processes. We focus primarily on a number of impactful publications coming from the Autonomous Systems Lab\footnote{\url{www.asl.ethz.ch}} (ASL) at ETH~Zurich\footnote{\textit{Eidgen{\"o}ssische Technische Hochschule Z{\"u}rich}, the Swiss Federal Institute of Technology in Zurich.} and the University of Pennsylvania's GRASP Lab\footnote{\url{www.grasp.upenn.edu}}. We define the current state of the art in filter-based navigation and establish the research context in which this thesis exists.

\subsection*{Algorithm Design and Implementation}

Because of the algorithmic nature of state estimation processes, we explore in detail the design and implementation of the \texttt{kalman\_sense} ROS package. We discuss plant model abstraction as well as code organization and data flow and then summarize the process by which one could extend \texttt{kalman\_sense}'s functionality and the advantages of system-agnostic algorithm design.

\subsection*{Experimental Design}

In this section, we first establish the goals of the testing regimen and then discuss the real-world execution of these goals. We discuss important statistical methods for characterizing the system's effectiveness as well as data collection procedures and post-processing. The system's physical testing infrastructure is explored in detail.

\subsection*{Experimental Results}

In Experimental Results, we evaluate the system's performance during testing and seek out any limiting factors that influence estimation accuracy. We probe for possible improvements to the algorithm and provide a notional understanding of the system's theoretical effectiveness in real-world scenarios.

\subsection*{Conclusions and Future Work}

We summarize the contributions made in this thesis, the effectiveness of the \texttt{kalman\_sense} ROS package, and the insights acquired during programming and testing. We explore avenues for future research as well as possible improvements to the existing system.