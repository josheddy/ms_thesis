\chapter{Conclusions and Future Work}

\section{Conclusions}

In the course of this research, we have developed an algorithm for fusion of IMU and SLAM data for localization of a small aircraft capable of hovering flight. We have presented not only an Unscented Kalman Filter formulation for this purpose, but also two corrective measures for maintaining quaternion continuity and deriving velocity measurements from pose readings to maintain numerical stability in the filter's output. We have also presented software implementation details related to the creation of a ROS package performing this fusion algorithm and described a number of architectural traits of this package encouraging and allowing for the adaptation of this work to systems other than small rotorcraft.

This work is largely motivated by the confluence of increased computing power and increased payload capacity for small UAS. The system developed in this thesis is meant to empower the roboticist researcher with a general state estimation framework useful for modeling a wide range of systems with arbitrary levels of fidelity. The experiments detailed in Chapters~\ref{ch:Exp_Design} and \ref{ch:Exp_Results} deployed the system in an indoor navigation scenario to characterize its behavior in a realistic environment where GPS would never be available as a backup sensing modality. In box pattern tests, the system consistently maintained a mean total position error of less than 10~cm with maximal errors never exceeding 25.96~cm. By these performance metrics, the UKF framework is more accurate than today's smartphone-grade GPS receivers and on par with similar sensor fusion systems detailed in Chapter~\ref{ch:Prior_Work}.

Nevertheless, the system faces several critical challenges. The PTAM implementation used during the experiments suffers from a number of serious defects. This PTAM implementation exhibits aberrant $z$-position estimates over flat terrain, erroneously conflates rotations with translations, and appears not to eliminate lens distortion. These defects severely limit the possible use cases for the system, assuming the same PTAM implementation is used. For effective use in non-laboratory scenarios, the vehicle would have to fly in pure translation (that is, without substantial rotations). Though this may seem like an odd requirement, modern hobby-grade autopilots, such as the Pixhawk, already allow for precisely this type of flight.

Perhaps the more stringent requirement on this system is that it be used in environments with ample visual geometry. The SLAM algorithm used in experimentation was susceptible to losing tracking if the density of visible features was not sufficiently high. This would be a necessary condition for any SLAM/VO algorithm, meaning that the system can only be deployed in areas with plenty of high-contrast features (and sufficient lighting by which to see them).

With all of this being said, the UKF framework developed here still represents a useful jumping-off point for further work. Future developments in this area (explored in detail in the next section) will likely revolve around increasing accuracy and robustness, the two principle challenges of any navigation system. Even the much-glorified Global Positioning System is not without its systemic faults and environmental limitations. Because of its generality, this work could be applied to great effect in numerous state estimation scenarios. Beyond rotorcraft and fixed-wing airplanes, this UKF framework could be adapted for use with spacecraft, surface vessels, undersea vehicles, and automobiles. Due to increasing demand for intelligent systems and unmanned vehicles of many varieties, it is likely that research in this area will continue for decades to come. In any case, the emphasis will likely remain on higher accuracy and greater robustness. With the growing trend of miniaturization and the increasing density of human populations, the need for ever more precise localization will always be present.

\section{Future Work}

\subsection{System Improvements}

Experimentation with the \texttt{kalman\_sense} ROS package has revealed a number of areas for improvement. Speaking generally, the system needs enhancements to improve accuracy and to become more robust with respect to sensor malfunction. Further development of this work likely would center on three main areas:
\begin{enumerate}
    \item Integration of new sensors,
    \item Improvements for current sensors, and
    \item Refinements to the underlying algorithm.
\end{enumerate}
Modifications to any or all of these areas would produce a marked improvement to the system in terms of both accuracy and robustness. For maximal effectiveness in adverse conditions, the system's sensors would need to be both individually robust and collectively redundant, so that complementary sensors could compensate for the loss of another sensor's information.

\subsubsection{Integration of New Sensors}
The accuracy and robustness of the system could be augmented by the integration of additional sensors, including but not limited to, the following:
\begin{enumerate}
    \item One or more GPS receivers (particularly Real Time Kinematic\footnote{\url{https://en.wikipedia.org/wiki/Real_Time_Kinematic}} GPS),
    \item A pressure altimeter,
    \item A 3-axis magnetometer,
    \item One or more laser rangefinders,
    \item One or more forward-facing cameras (preferably depth cameras, such as RGB-D),
    \item One-dimensional, multi-dimensional, or scanning LIDAR\footnote{``Light Detection and Ranging,'' \url{https://en.wikipedia.org/wiki/Lidar}}.
\end{enumerate}

We previously mentioned work from the GRASP Lab (\cite{Shen2011}) that demonstrated the efficacy of similar UKF frameworks in indoor-outdoor transitions and in navigating confined spaces using an expanded suite of sensors. In the case where more than one sensor is used to observe a given vehicle state variable, those multiple sensors check one another and can even perform real-time, in-air calibration. The system developed in this thesis has the advantage of being hardware-minimal in that it depends on only two sensors (the IMU and the ventral camera), but this convenience comes at the cost of robustness because either sensor presents a single point of possible failure. Moreover, the sensors have no overlap. Neither can observe any of the states observed by the other. This eliminates the possibility of checking one sensor against the other. Introducing a ventral laser rangefinder or pressure altimeter would allow for much more precise measurement of the vehicle's altitude and would allow for in-flight re-initialization of any metric SLAM/VO algorithm.

With the exception of certain sensors such as scanning LIDAR, which can cost tens of thousands of dollars, and RTK-GPS, which often falls in the same price range, these proposed new sensors would be inexpensive additions to the vehicle. Given the sophistication of current miniaturized sensors, it is not unreasonable to expect sub-meter or centimeter-level position accuracy from vehicles weighing fewer than fifteen pounds (including a substantial sensor payload) and costing less than 20,000~USD. It has been the case for a number of years now that meter-level accuracy could be taken for granted with ``toy'' quadcopters using only MEMS\footnote{``Microelectromechanical systems,''\\ \url{https://en.wikipedia.org/wiki/Microelectromechanical_systems}} IMUs and sub-\$100 GPS receivers. The trend of miniaturization has made more and more sensors of better and better quality increasingly available in the weight- and cost-constrained world of aerial robotics. A variety of inexpensive off-the-shelf autopilots, such as the Pixhawk~2.1\footnote{\url{http://www.proficnc.com/}}, already boast triple-redundant IMUs and support for multiple GPS modules. Many of the hardware integration challenges that once plagued hobbyists and researchers have been resolved by engineering developments stemming from popular demand for user-friendly drones.

The organization of the \texttt{kalman\_sense} package's code base is such that the integration of new sensors would require only the addition of new callback functions and ROS subscribers, as opposed to a full redesign or major edits to the existing code. Further, a new SLAM or VO algorithm could also be used in lieu of PTAM, allowing an ``apples-to-apples'' comparison of different visual localization algorithms. For example, Donavanik et~al.\ \cite{Donavanik2016} have identified a new algorithm known as ORB-SLAM \cite{Mur-Artal2015} as a promising candidate for robust SLAM, already implemented in ROS. Because of ROS's sophisticated ecosystem of interoperable packages and message formats, any pose sensor employing the same message type as PTAM could plug in to \texttt{kalman\_sense} in a matter of seconds.

Much of the design thinking that went into the \texttt{kalman\_sense} package was focused on this particular possibility. The aircraft of the future will be vehicles laden with a vast suite of heterogeneous sensors, the outputs of which would, at any given time, be used not only for inner-loop and outer-loop flight control, but also for self-calibration, real-time diagnostics, and operator telemetry. Future UAVs will be akin to floating laboratories, mobile arrays of cutting-edge measurement systems allowing remote sensing over areas of several square miles.

\subsubsection{Improvements for Current Sensors}

For improved pose measurements, a more robust implementation of PTAM could be written and the same experiments could be repeated for comparison. PTAM's rotation errors and inability to eliminate lens distortion constitute severe limitations to its effectiveness in small UAS navigation. Moreover, if a rewritten formulation were able to interface with a downward-facing rangefinder, the initialization process could be eliminated altogether. The drawback is that making these changes would require a major software rewrite, a project that could be feasibly undertaken only by an experienced team of computer vision researchers and software developers. The mere cost in terms of both time and labor would likely make this an unappealing proposition, especially if other ready-made SLAM/VO algorithms were available to serve as PTAM replacements.

Processing power will also be central to the system's effectiveness during deployment on a real aircraft. This research is in many ways the product of recent advances in computing power. The advent of miniature desktop computers, such as the Intel~NUC\footnote{``Next Unit of Computing,'' \url{https://en.wikipedia.org/wiki/Next_Unit_of_Computing}}, as well as power-efficient Graphics Processing Units\footnote{\url{https://en.wikipedia.org/wiki/Graphics_processing_unit}} (GPUs) has made a new frontier of estimation and localization algorithms easily accessible for the first time in the UAV world. A simple and generally inexpensive approach to augmenting the \texttt{kalman\_sense} system would thus be the inclusion of top-of-the-line lightweight computing hardware. With modern onboard computers, even the need for cross-platform code compilation (so-called ``transpiling'') is eliminated because these computers employ the same x86 architecture used by most software developers.

\subsubsection{Refinements to the Underlying Algorithm}

The UKF formulation itself presents another set of opportunities for improvement. In particular, better process and measurement noise modeling would allow for increased accuracy. As described previously, the noise covariance matrices used during the experiments were only rough, static models meant to overestimate the Gaussian noise in the system. In practice, it would be better to replace these with time-varying matrices with terms more closely matching the intrinsic parameters of the system. Due to the high degree of coupling between state variables and the fundamental nonlinearity of the process model, this type of work was considered to be outside the scope of my thesis. In the future, however, a better $\mathbf{Q}$ matrix could be determined, perhaps by application of autocovariance and system identification methods. Moreover, any new sensor integrated into the system would require its own $\mathbf{R}$ matrix detailing its particular idiosyncrasies. With this done, attention could then be turned to tuning the $\mathbf{Q} / \mathbf{R}$ ratio of each sensor, thus making the system even more faithful to known sensor parameters. With more sensors, self-calibration techniques such as those employed in \cite{Weiss2012} could also be implemented to avoid the process of reinitializing one or more sensors in mid-air.

\subsection{Future Experiments}

Using a SLAM/VO algorithm that addresses pure rotation and lens distortion would be the easiest and most cost effective way to improve the system. Testing different visual localization algorithms would thus be an appealing avenue of inquiry. It would behoove the researcher to characterize the system's behavior under the same conditions that caused the PTAM-based implementation to fail. In particular, experiments that include pure rotations and longer translations, as well as simulated flight movement over uneven but otherwise virtually unchanging terrain would mimic real-world flight missions more closely and provide a more holistic understanding of the system's effectiveness. Additionally, mounting the sensor suite on the underside of a UAV would allow for the collection of real-world flight data, which could shed light on the system's speed limits and behavior in the presence of in-flight vibrations.

After integrating new sensors, new experiments could be devised to characterize the system's effectiveness during sensor blackouts---for example, during an indoor-outdoor transition after integrating a GPS receiver. Experiments such as this could then inform the design of heuristic models for recognizing sensor degradation and perhaps even consensus-based estimation strategies. 

\subsection{System Applications}

Applications for this system are wide-ranging, spanning virtually every conceivable use for a small drone. The following is a non-exhaustive list of possible applications:
\begin{enumerate}
    \item Mapping and 3D reconstruction of structures and topography,
    \item Emergency response,
    \item Infrastructure inspection,
    \item Autonomous package delivery, and
    \item Military/defense solutions.
\end{enumerate}
What all of these applications have in common is the need for robust localization. In each of the above scenarios, losing GPS could cause a crash. In such an event, the vehicle itself would pose a real threat to bystanders and property. Moreover, in the case of military aircraft, the vehicle could be captured by hostile forces.

\subsubsection{Mapping and 3D Reconstruction}

Growing demand for so-called ``precision agriculture'' has brought with it the need to map large areas of cropland at a high speed, with a low cost point. In the past, this need has been met by using satellite imagery and human-captured photography. With the arrival of low-cost drones, this work has been offloaded to aerial robots. Outfitted with specialized sensors such as hyperspectral cameras\footnote{\url{https://en.wikipedia.org/wiki/Hyperspectral_imaging}}, small UAS are now able to provide minute-by-minute coverage of large expanses of land. More interesting still, these robots can be outfitted with reservoirs of pesticide or fertilizer and deployed to spray individual plants autonomously. It is in roles such as this that high-accuracy robust localization becomes a necessity. The accuracy required for precision agriculture can be provided by utilizing the sensor fusion techniques discussed in this thesis.

The surfaces of buildings and other structures can be mapped in a similar fashion, given the appropriate sensors and the right framework by which to fuse them. 3D reconstruction has become popular among real estate agents, safety inspectors, and other parties with a vested interest in precise 3D renderings of large geometries. In this case, the aircraft must be able to localize itself precisely in order to take full advantage of onboard sense-and-avoid technologies. The ability of the aircraft to estimate its position is crucial to its ability to map hard-to-reach areas and maintain stable flight. Sensor fusion systems shine when GPS and other sensors are inevitably compromised by various environmental factors.

\subsubsection{Emergency Response}

UAS technologies have become a desirable tool for first responders in many parts of the world. The ability to deploy a UAV to survey forest fires or search for missing persons has been a game changer for law enforcement and emergency response teams. In the case of looking for survivors of a natural disaster, the requirements for an effective vehicle system are accurate localization and overall robustness. Vehicles sent into disaster areas will have to be able to navigate in sensor-compromising environments, sometimes at substantial speed. This use case highlights the need for redundant sensors and intelligent, failure-resistant sensor fusion.

\subsubsection{Infrastructure Inspection}

Infrastructure inspection is similar in several respects to 3D mapping. In both cases, small drones are preferable to manned aircraft because of their low cost, high maneuverability, and ease of use. In recent years, governments and private corporations have taken an interest in using small aircraft to automate the inspection of many types of infrastructure, including the following:
\begin{enumerate}
    \item Power lines,
    \item Oil rigs,
    \item Gas pipelines,
    \item Railroads, and
    \item Buildings.
\end{enumerate}
UAVs are an obvious ally to companies servicing most types of static infrastructure. Inspecting miles of wires or railroad track is a task well-suited to today's UAVs, which commonly ship with intuitive flight planning software. Operators---even those who have no experience flying remote-control aircraft---are now able to program missions and deploy drones with ease. The aircraft are able to survey miles of infrastructure regardless of conditions on the ground, such as flooding, ill-maintained roads, or wild animals. Operators with First-Person View (FPV) hardware can watch video streaming live from the aircraft as if they were onboard themselves. The need for robust localization here is obvious: the UAV cannot inspect anything to which it cannot navigate. Operating in the wilderness may require flying below the forest canopy or under natural overhangs that could block GPS reception. The same is true in urban environments, where tall buildings create so-called ``urban canyons'' which compromise GPS effectiveness. Large metal structures can also distort magnetometry readings and thus undermine the aircraft's ability to maintain its heading. In any case, UAVs used for large-scale inspections will be dependent upon intelligently combined sensing modalities.

\subsubsection{Autonomous Package Delivery}

A number of organizations, most notably Amazon\footnote{\url{https://www.amazon.com/Amazon-Prime-Air/b?node=8037720011}}, have made big bets on drones as the future of delivery services. The task of delivering goods to individual homes is no trifling matter. American homes come in all shapes and sizes and are surrounded by numerous structures and natural obstacles that could endanger a delivery aircraft and, by extension, the people and property nearby. Moreover, any drone delivery service would have to be predicated upon the ability of the UAV to land with sub-meter accuracy. Delivery drones will have to land precisely in cluttered environments amid dynamic obstacles. Visual sensing will be extremely important for identifying safe landing zones, avoiding power lines and fences, and surmounting other environmental challenges. GPS alone will not be enough to guide the vehicle safely in every conceivable circumstance. 

\subsubsection{Military/Defense Solutions}

Perhaps the most obvious application of a UKF fusion framework is in military operations, where precision navigation and robustness to sensor degradation are paramount for protecting soldiers in the field. As of the time of this writing, UAVs have been used in combat for years. However, augmented navigation could still make revolutionary contributions in areas such as
\begin{enumerate}
    \item Robust tracking of friendly and hostile elements,
    \item Remote observation of roads and vehicle-related hazards,
    \item Autonomous deliveries of materiel in-theater, and
    \item Human-machine teaming.
\end{enumerate}
Every combat zone will constitute a hostile environment for UAV operations, especially in the presence of GPS spoofing/jamming technologies. Combat UAVs will have to be able to tolerate multiple simultaneous sensor failures. Moreover, in-theater urban flight operations bring with them all of the same difficulties mentioned above, but with the added challenge of violent enemy interdiction. Robust sensor fusion could automate the task of operational overwatch and provide mission-critical intelligence in a timely manner---but only if the vehicle can localize itself reliably in sensor-hostile environments.